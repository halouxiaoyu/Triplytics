"""
ç©ºé—´åˆ†ææ¨¡å—
æä¾›åœ°ç†ç©ºé—´æ•°æ®çš„åˆ†æå’Œå¯è§†åŒ–åŠŸèƒ½
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Optional, Tuple, Any
import warnings
from scipy.spatial.distance import cdist
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
import folium
from folium.plugins import HeatMap

from config.logging_config import LoggerMixin, log_execution_time
from config.settings import NYC_BOUNDS, CLUSTERING_CONFIG

warnings.filterwarnings('ignore')

class SpatialAnalysis(LoggerMixin):
    """
    ç©ºé—´åˆ†æç±»
    æä¾›åœ°ç†ç©ºé—´æ•°æ®çš„åˆ†æåŠŸèƒ½
    """
    
    def __init__(self, df: pd.DataFrame):
        """
        åˆå§‹åŒ–ç©ºé—´åˆ†æå™¨
        
        Args:
            df: åŒ…å«åœ°ç†åæ ‡çš„DataFrame
        """
        super().__init__()
        self.df = df.copy()
        self.log_info(f"SpatialAnalysis initialized with data shape: {df.shape}")
        
        # è¯†åˆ«åæ ‡åˆ—
        self.lat_col = self._find_lat_column()
        self.lon_col = self._find_lon_column()
        
        if self.lat_col is None or self.lon_col is None:
            self.log_warning("Latitude or longitude columns not found")
    
    def _find_lat_column(self) -> Optional[str]:
        """æŸ¥æ‰¾çº¬åº¦åˆ—"""
        lat_candidates = ['latitude', 'Lat', 'lat']
        for col in lat_candidates:
            if col in self.df.columns:
                return col
        return None
    
    def _find_lon_column(self) -> Optional[str]:
        """æŸ¥æ‰¾ç»åº¦åˆ—"""
        lon_candidates = ['longitude', 'Lon', 'lon']
        for col in lon_candidates:
            if col in self.df.columns:
                return col
        return None
    
    @log_execution_time
    def analyze_geographic_distribution(self) -> Dict[str, Any]:
        """
        åˆ†æåœ°ç†åˆ†å¸ƒ
        
        Returns:
            åœ°ç†åˆ†å¸ƒåˆ†æç»“æœ
        """
        self.log_info("Analyzing geographic distribution")
        
        if self.lat_col is None or self.lon_col is None:
            return {'error': 'Latitude or longitude columns not found'}
        
        analysis = {
            'bounds': self._calculate_bounds(),
            'center': self._calculate_center(),
            'density': self._calculate_density(),
            'spread': self._calculate_spread(),
            'outliers': self._detect_geographic_outliers()
        }
        
        self.log_info("Geographic distribution analysis completed")
        return analysis
    
    def _calculate_bounds(self) -> Dict[str, Tuple[float, float]]:
        """è®¡ç®—åœ°ç†è¾¹ç•Œ"""
        return {
            'latitude': (self.df[self.lat_col].min(), self.df[self.lat_col].max()),
            'longitude': (self.df[self.lon_col].min(), self.df[self.lon_col].max()),
            'nyc_bounds': NYC_BOUNDS
        }
    
    def _calculate_center(self) -> Dict[str, float]:
        """è®¡ç®—åœ°ç†ä¸­å¿ƒ"""
        return {
            'latitude': self.df[self.lat_col].mean(),
            'longitude': self.df[self.lon_col].mean(),
            'median_lat': self.df[self.lat_col].median(),
            'median_lon': self.df[self.lon_col].median()
        }
    
    def _calculate_density(self) -> Dict[str, Any]:
        """è®¡ç®—å¯†åº¦ç»Ÿè®¡"""
        # è®¡ç®—æ¯ä¸ªç‚¹å‘¨å›´çš„ç‚¹æ•°
        coords = self.df[[self.lat_col, self.lon_col]].values
        
        # ä¸ºäº†é¿å…è®¡ç®—é‡è¿‡å¤§ï¼Œé‡‡æ ·è®¡ç®—
        sample_size = min(1000, len(coords))
        sample_coords = coords[:sample_size]
        
        # è®¡ç®—è·ç¦»çŸ©é˜µ
        distances = cdist(sample_coords, coords)
        
        # è®¡ç®—ä¸åŒåŠå¾„å†…çš„ç‚¹æ•°
        radius_1km = 0.01  # çº¦1å…¬é‡Œ
        radius_5km = 0.05  # çº¦5å…¬é‡Œ
        
        density_1km = (distances < radius_1km).sum(axis=1) - 1  # å‡å»è‡ªå·±
        density_5km = (distances < radius_5km).sum(axis=1) - 1
        
        return {
            'density_1km_mean': density_1km.mean(),
            'density_1km_std': density_1km.std(),
            'density_1km_max': density_1km.max(),
            'density_5km_mean': density_5km.mean(),
            'density_5km_std': density_5km.std(),
            'density_5km_max': density_5km.max()
        }
    
    def _calculate_spread(self) -> Dict[str, float]:
        """è®¡ç®—åœ°ç†åˆ†å¸ƒèŒƒå›´"""
        return {
            'lat_range': self.df[self.lat_col].max() - self.df[self.lat_col].min(),
            'lon_range': self.df[self.lon_col].max() - self.df[self.lon_col].min(),
            'lat_std': self.df[self.lat_col].std(),
            'lon_std': self.df[self.lon_col].std()
        }
    
    def _detect_geographic_outliers(self) -> Dict[str, Any]:
        """æ£€æµ‹åœ°ç†å¼‚å¸¸å€¼"""
        # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
        lat_q1 = self.df[self.lat_col].quantile(0.25)
        lat_q3 = self.df[self.lat_col].quantile(0.75)
        lat_iqr = lat_q3 - lat_q1
        
        lon_q1 = self.df[self.lon_col].quantile(0.25)
        lon_q3 = self.df[self.lon_col].quantile(0.75)
        lon_iqr = lon_q3 - lon_q1
        
        lat_outliers = self.df[
            (self.df[self.lat_col] < lat_q1 - 1.5 * lat_iqr) |
            (self.df[self.lat_col] > lat_q3 + 1.5 * lat_iqr)
        ]
        
        lon_outliers = self.df[
            (self.df[self.lon_col] < lon_q1 - 1.5 * lon_iqr) |
            (self.df[self.lon_col] > lon_q3 + 1.5 * lon_iqr)
        ]
        
        return {
            'lat_outliers_count': len(lat_outliers),
            'lon_outliers_count': len(lon_outliers),
            'total_outliers': len(pd.concat([lat_outliers, lon_outliers]).drop_duplicates())
        }
    
    @log_execution_time
    def perform_clustering_analysis(self, method: str = 'kmeans', **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œèšç±»åˆ†æ
        
        Args:
            method: èšç±»æ–¹æ³• ('kmeans', 'dbscan')
            **kwargs: èšç±»å‚æ•°
            
        Returns:
            èšç±»åˆ†æç»“æœ
        """
        self.log_info(f"Performing {method} clustering analysis")
        
        if self.lat_col is None or self.lon_col is None:
            return {'error': 'Latitude or longitude columns not found'}
        
        coords = self.df[[self.lat_col, self.lon_col]].values
        
        if method == 'kmeans':
            return self._kmeans_clustering(coords, **kwargs)
        elif method == 'dbscan':
            return self._dbscan_clustering(coords, **kwargs)
        else:
            self.log_error(f"Unknown clustering method: {method}")
            return {'error': f'Unknown clustering method: {method}'}
    
    def _kmeans_clustering(self, coords: np.ndarray, n_clusters: int = 8) -> Dict[str, Any]:
        """K-meansèšç±»"""
        kmeans_config = CLUSTERING_CONFIG['kmeans'].copy()
        kmeans_config['n_clusters'] = n_clusters
        
        kmeans = KMeans(**kmeans_config)
        cluster_labels = kmeans.fit_predict(coords)
        
        # æ·»åŠ èšç±»æ ‡ç­¾åˆ°æ•°æ®
        self.df[f'cluster_{n_clusters}'] = cluster_labels
        
        # åˆ†æèšç±»ç»“æœ
        cluster_counts = pd.Series(cluster_labels).value_counts()
        cluster_centers = kmeans.cluster_centers_
        
        # è®¡ç®—æ¯ä¸ªèšç±»çš„ç»Ÿè®¡ä¿¡æ¯
        cluster_stats = {}
        for i in range(n_clusters):
            cluster_data = self.df[cluster_labels == i]
            cluster_stats[i] = {
                'count': len(cluster_data),
                'percentage': (len(cluster_data) / len(self.df)) * 100,
                'center_lat': cluster_centers[i, 0],
                'center_lon': cluster_centers[i, 1],
                'radius': self._calculate_cluster_radius(cluster_data, cluster_centers[i])
            }
        
        return {
            'method': 'kmeans',
            'n_clusters': n_clusters,
            'cluster_counts': cluster_counts.to_dict(),
            'cluster_centers': cluster_centers.tolist(),
            'cluster_stats': cluster_stats,
            'inertia': kmeans.inertia_,
            'model': kmeans
        }
    
    def _dbscan_clustering(self, coords: np.ndarray, eps: float = 0.01, min_samples: int = 50) -> Dict[str, Any]:
        """DBSCANèšç±»"""
        dbscan_config = CLUSTERING_CONFIG['dbscan'].copy()
        dbscan_config.update({'eps': eps, 'min_samples': min_samples})
        
        dbscan = DBSCAN(**dbscan_config)
        cluster_labels = dbscan.fit_predict(coords)
        
        # æ·»åŠ èšç±»æ ‡ç­¾åˆ°æ•°æ®
        self.df['cluster_dbscan'] = cluster_labels
        
        # åˆ†æèšç±»ç»“æœ
        cluster_counts = pd.Series(cluster_labels).value_counts()
        n_clusters = len(cluster_counts) - (1 if -1 in cluster_labels else 0)  # æ’é™¤å™ªå£°ç‚¹
        
        # è®¡ç®—æ¯ä¸ªèšç±»çš„ç»Ÿè®¡ä¿¡æ¯
        cluster_stats = {}
        for cluster_id in cluster_counts.index:
            if cluster_id == -1:  # å™ªå£°ç‚¹
                cluster_stats[cluster_id] = {
                    'count': cluster_counts[cluster_id],
                    'percentage': (cluster_counts[cluster_id] / len(self.df)) * 100,
                    'type': 'noise'
                }
            else:
                cluster_data = self.df[cluster_labels == cluster_id]
                cluster_center = cluster_data[[self.lat_col, self.lon_col]].mean().values
                cluster_stats[cluster_id] = {
                    'count': len(cluster_data),
                    'percentage': (len(cluster_data) / len(self.df)) * 100,
                    'center_lat': cluster_center[0],
                    'center_lon': cluster_center[1],
                    'radius': self._calculate_cluster_radius(cluster_data, cluster_center)
                }
        
        return {
            'method': 'dbscan',
            'n_clusters': n_clusters,
            'cluster_counts': cluster_counts.to_dict(),
            'cluster_stats': cluster_stats,
            'noise_points': cluster_counts.get(-1, 0),
            'model': dbscan
        }
    
    def _calculate_cluster_radius(self, cluster_data: pd.DataFrame, center: np.ndarray) -> float:
        """è®¡ç®—èšç±»åŠå¾„"""
        if len(cluster_data) == 0:
            return 0.0
        
        coords = cluster_data[[self.lat_col, self.lon_col]].values
        distances = np.sqrt(np.sum((coords - center) ** 2, axis=1))
        return distances.mean()
    
    @log_execution_time
    def analyze_hotspots(self, time_window: Optional[str] = None) -> Dict[str, Any]:
        """
        åˆ†æçƒ­ç‚¹åŒºåŸŸ
        
        Args:
            time_window: æ—¶é—´çª—å£ ('hour', 'day', 'week')
            
        Returns:
            çƒ­ç‚¹åˆ†æç»“æœ
        """
        self.log_info("Analyzing hotspots")
        
        if self.lat_col is None or self.lon_col is None:
            return {'error': 'Latitude or longitude columns not found'}
        
        hotspots = {}
        
        if time_window is None:
            # æ•´ä½“çƒ­ç‚¹
            hotspots['overall'] = self._calculate_hotspots()
        else:
            # æŒ‰æ—¶é—´çª—å£åˆ†æçƒ­ç‚¹
            if time_window in self.df.columns:
                for value in self.df[time_window].unique():
                    subset = self.df[self.df[time_window] == value]
                    hotspots[f'{time_window}_{value}'] = self._calculate_hotspots(subset)
        
        return hotspots
    
    def _calculate_hotspots(self, data: Optional[pd.DataFrame] = None) -> Dict[str, Any]:
        """è®¡ç®—çƒ­ç‚¹"""
        if data is None:
            data = self.df
        
        # åˆ›å»ºç½‘æ ¼
        lat_bins = np.linspace(data[self.lat_col].min(), data[self.lat_col].max(), 50)
        lon_bins = np.linspace(data[self.lon_col].min(), data[self.lon_col].max(), 50)
        
        # è®¡ç®—2Dç›´æ–¹å›¾
        hist, lat_edges, lon_edges = np.histogram2d(
            data[self.lat_col], data[self.lon_col], 
            bins=[lat_bins, lon_bins]
        )
        
        # æ‰¾åˆ°çƒ­ç‚¹ï¼ˆé«˜å¯†åº¦åŒºåŸŸï¼‰
        threshold = np.percentile(hist[hist > 0], 90)  # å‰10%çš„é«˜å¯†åº¦åŒºåŸŸ
        hotspot_indices = np.where(hist >= threshold)
        
        hotspots = []
        for i, j in zip(hotspot_indices[0], hotspot_indices[1]):
            hotspots.append({
                'lat_center': (lat_edges[i] + lat_edges[i+1]) / 2,
                'lon_center': (lon_edges[j] + lon_edges[j+1]) / 2,
                'density': hist[i, j]
            })
        
        return {
            'hotspots': hotspots,
            'max_density': hist.max(),
            'mean_density': hist.mean(),
            'hotspot_count': len(hotspots)
        }
    
    @log_execution_time
    def create_spatial_visualizations(self, save_path: Optional[str] = None) -> Dict[str, str]:
        """
        åˆ›å»ºç©ºé—´å¯è§†åŒ–
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
            
        Returns:
            å›¾è¡¨æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        self.log_info("Creating spatial visualizations")
        
        if self.lat_col is None or self.lon_col is None:
            self.log_error("Latitude or longitude columns not found")
            return {}
        
        saved_files = {}
        
        # 1. æ•£ç‚¹å›¾
        plt.figure(figsize=(12, 8))
        sample_size = min(10000, len(self.df))
        sample_df = self.df.sample(n=sample_size, random_state=42)
        
        scatter = plt.scatter(sample_df[self.lon_col], sample_df[self.lat_col], 
                            c=sample_df['Base'].astype('category').cat.codes if 'Base' in sample_df.columns else 'blue',
                            s=1, alpha=0.6, cmap='tab10')
        plt.title('Geographic Distribution of Orders')
        plt.xlabel('Longitude')
        plt.ylabel('Latitude')
        plt.colorbar(scatter, label='Base Station')
        plt.grid(True, alpha=0.3)
        
        if save_path:
            scatter_path = f"{save_path}/geographic_scatter.png"
            plt.savefig(scatter_path, dpi=300, bbox_inches='tight')
            saved_files['scatter'] = scatter_path
        
        plt.show()
        
        # 2. çƒ­åŠ›å›¾
        plt.figure(figsize=(12, 8))
        plt.hist2d(sample_df[self.lon_col], sample_df[self.lat_col], bins=50, cmap='Reds')
        plt.title('Order Density Heatmap')
        plt.xlabel('Longitude')
        plt.ylabel('Latitude')
        plt.colorbar(label='Order Density')
        
        if save_path:
            heatmap_path = f"{save_path}/density_heatmap.png"
            plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')
            saved_files['heatmap'] = heatmap_path
        
        plt.show()
        
        # 3. èšç±»å¯è§†åŒ–
        if 'cluster_8' in self.df.columns:
            plt.figure(figsize=(12, 8))
            for cluster_id in self.df['cluster_8'].unique():
                cluster_data = self.df[self.df['cluster_8'] == cluster_id]
                plt.scatter(cluster_data[self.lon_col], cluster_data[self.lat_col], 
                          s=1, alpha=0.6, label=f'Cluster {cluster_id}')
            
            plt.title('Geographic Clusters')
            plt.xlabel('Longitude')
            plt.ylabel('Latitude')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            if save_path:
                cluster_path = f"{save_path}/geographic_clusters.png"
                plt.savefig(cluster_path, dpi=300, bbox_inches='tight')
                saved_files['clusters'] = cluster_path
            
            plt.show()
        
        self.log_info(f"Spatial visualizations created. Saved {len(saved_files)} files.")
        return saved_files
    
    def create_interactive_map(self, save_path: Optional[str] = None) -> str:
        """
        åˆ›å»ºäº¤äº’å¼åœ°å›¾
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
            
        Returns:
            åœ°å›¾æ–‡ä»¶è·¯å¾„
        """
        self.log_info("Creating interactive map")
        
        if self.lat_col is None or self.lon_col is None:
            self.log_error("Latitude or longitude columns not found")
            return ""
        
        # è®¡ç®—åœ°å›¾ä¸­å¿ƒ
        center_lat = self.df[self.lat_col].mean()
        center_lon = self.df[self.lon_col].mean()
        
        # åˆ›å»ºåœ°å›¾
        m = folium.Map(location=[center_lat, center_lon], zoom_start=12)
        
        # æ·»åŠ çƒ­åŠ›å›¾
        sample_size = min(5000, len(self.df))
        sample_df = self.df.sample(n=sample_size, random_state=42)
        
        heat_data = sample_df[[self.lat_col, self.lon_col]].values.tolist()
        HeatMap(heat_data, radius=15).add_to(m)
        
        # æ·»åŠ èšç±»ä¸­å¿ƒï¼ˆå¦‚æœæœ‰èšç±»ç»“æœï¼‰
        if 'cluster_8' in self.df.columns:
            cluster_centers = self.df.groupby('cluster_8')[[self.lat_col, self.lon_col]].mean()
            
            for cluster_id, center in cluster_centers.iterrows():
                folium.Marker(
                    location=[center[self.lat_col], center[self.lon_col]],
                    popup=f'Cluster {cluster_id}',
                    icon=folium.Icon(color='red', icon='info-sign')
                ).add_to(m)
        
        # ä¿å­˜åœ°å›¾
        if save_path:
            map_path = f"{save_path}/interactive_map.html"
            m.save(map_path)
            self.log_info(f"Interactive map saved to: {map_path}")
            return map_path
        
        return m
    
    def analyze_spatial_temporal_patterns(self) -> Dict[str, Any]:
        """
        åˆ†ææ—¶ç©ºæ¨¡å¼
        
        Returns:
            æ—¶ç©ºæ¨¡å¼åˆ†æç»“æœ
        """
        self.log_info("Analyzing spatial-temporal patterns")
        
        if self.lat_col is None or self.lon_col is None:
            return {'error': 'Latitude or longitude columns not found'}
        
        patterns = {}
        
        # æŒ‰å°æ—¶åˆ†æç©ºé—´åˆ†å¸ƒ
        if 'hour' in self.df.columns:
            hourly_patterns = {}
            for hour in range(24):
                hour_data = self.df[self.df['hour'] == hour]
                if len(hour_data) > 0:
                    hourly_patterns[hour] = {
                        'count': len(hour_data),
                        'center_lat': hour_data[self.lat_col].mean(),
                        'center_lon': hour_data[self.lon_col].mean(),
                        'spread': hour_data[self.lat_col].std() + hour_data[self.lon_col].std()
                    }
            patterns['hourly'] = hourly_patterns
        
        # æŒ‰æ˜ŸæœŸåˆ†æç©ºé—´åˆ†å¸ƒ
        if 'weekday' in self.df.columns:
            weekday_patterns = {}
            for day in range(7):
                day_data = self.df[self.df['weekday'] == day]
                if len(day_data) > 0:
                    weekday_patterns[day] = {
                        'count': len(day_data),
                        'center_lat': day_data[self.lat_col].mean(),
                        'center_lon': day_data[self.lon_col].mean(),
                        'spread': day_data[self.lat_col].std() + day_data[self.lon_col].std()
                    }
            patterns['weekday'] = weekday_patterns
        
        return patterns
    
    def print_spatial_summary(self):
        """æ‰“å°ç©ºé—´åˆ†ææ‘˜è¦"""
        analysis = self.analyze_geographic_distribution()
        
        print("=" * 80)
        print("ç©ºé—´åˆ†ææ‘˜è¦")
        print("=" * 80)
        
        if 'error' in analysis:
            print(f"âŒ é”™è¯¯: {analysis['error']}")
            return
        
        # åœ°ç†è¾¹ç•Œ
        bounds = analysis['bounds']
        print(f"\nğŸ“ åœ°ç†è¾¹ç•Œ:")
        print(f"  çº¬åº¦èŒƒå›´: {bounds['latitude']}")
        print(f"  ç»åº¦èŒƒå›´: {bounds['longitude']}")
        
        # åœ°ç†ä¸­å¿ƒ
        center = analysis['center']
        print(f"\nğŸ¯ åœ°ç†ä¸­å¿ƒ:")
        print(f"  å¹³å‡ä¸­å¿ƒ: ({center['latitude']:.4f}, {center['longitude']:.4f})")
        print(f"  ä¸­ä½æ•°ä¸­å¿ƒ: ({center['median_lat']:.4f}, {center['median_lon']:.4f})")
        
        # å¯†åº¦ç»Ÿè®¡
        density = analysis['density']
        print(f"\nğŸ“Š å¯†åº¦ç»Ÿè®¡:")
        print(f"  1kmèŒƒå›´å†…å¹³å‡ç‚¹æ•°: {density['density_1km_mean']:.1f}")
        print(f"  5kmèŒƒå›´å†…å¹³å‡ç‚¹æ•°: {density['density_5km_mean']:.1f}")
        
        # å¼‚å¸¸å€¼
        outliers = analysis['outliers']
        print(f"\nâš ï¸ åœ°ç†å¼‚å¸¸å€¼:")
        print(f"  çº¬åº¦å¼‚å¸¸å€¼: {outliers['lat_outliers_count']}")
        print(f"  ç»åº¦å¼‚å¸¸å€¼: {outliers['lon_outliers_count']}")
        print(f"  æ€»å¼‚å¸¸å€¼: {outliers['total_outliers']}")
        
        print("\n" + "=" * 80)

# ä¾¿æ·å‡½æ•°
def analyze_spatial_patterns(df: pd.DataFrame) -> Dict[str, Any]:
    """
    å¿«é€Ÿç©ºé—´æ¨¡å¼åˆ†æ
    
    Args:
        df: DataFrame
        
    Returns:
        ç©ºé—´åˆ†æç»“æœ
    """
    spatial_analyzer = SpatialAnalysis(df)
    return spatial_analyzer.analyze_geographic_distribution()

def create_spatial_clusters(df: pd.DataFrame, n_clusters: int = 8) -> Dict[str, Any]:
    """
    åˆ›å»ºç©ºé—´èšç±»
    
    Args:
        df: DataFrame
        n_clusters: èšç±»æ•°é‡
        
    Returns:
        èšç±»ç»“æœ
    """
    spatial_analyzer = SpatialAnalysis(df)
    return spatial_analyzer.perform_clustering_analysis('kmeans', n_clusters=n_clusters)

def create_spatial_visualizations(df: pd.DataFrame, save_path: Optional[str] = None) -> Dict[str, str]:
    """
    åˆ›å»ºç©ºé—´å¯è§†åŒ–
    
    Args:
        df: DataFrame
        save_path: ä¿å­˜è·¯å¾„
        
    Returns:
        å›¾è¡¨æ–‡ä»¶è·¯å¾„
    """
    spatial_analyzer = SpatialAnalysis(df)
    return spatial_analyzer.create_spatial_visualizations(save_path) 